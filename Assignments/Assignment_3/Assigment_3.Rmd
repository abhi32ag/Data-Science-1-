---
title: "Assignment_3"
author: "Abhinav Garg"
date: "2/12/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1
For each of the following situations, state whether the parameter of interest is a mean or a proportion. It may be helpful to examine whether individual responses are numerical or categorical.

a) (1 pt) In a survey, one hundred college students are asked how many hours per week they spend on the Internet.

Ans: These responses are numerical and the parameter of interest is a mean 

b) (1 pt) In a survey, one hundred college students are asked: “What percentage of the time you spend on the Internet is part of your course work?”

Ans: These responses are numerical. Therefore parameter of interest would be proportion 

c) (1 pt) In a survey, one hundred college students are asked whether or not they cited information from Wikipedia in their papers.

Ans: These responses would be categorical as they would be answered in yes / no / no response. This parameter would be proportion 

d) (1 pt) In a survey, one hundred college students are asked what percentage of their total weekly spending is on alcoholic beverages.

Ans: The parameter of interest here is proportion 

e) (1 pt) In a sample of one hundred recent college graduates, it is found that 85 percent expect to get a job within one year of their graduation date.

Ans: The parameter of interest here is proportion 

## 2

(5 pt) In 2013, the Pew Research Foundation reported that “45% of U.S. adults report that they live with one or more chronic conditions”. However, this value was based on a sample, so it may not be a perfect estimate for the population parameter of interest on its own. The study reported a standard error of about 1.2%, and a normal model may reasonably be used in this setting. Create a 95% confidence interval for the proportion of U.S. adults who live with one or more chronic conditions. Also interpret the confidence interval in the context of the study.

```{r 2}
u <- 45
se <- 1.2
left <- 45 - 1.96*se
right <- 45 + 1.96*se
left
```

```{r 2.a}
right 
```
So as we can see the confidence interval is between (42.648, 47.352)
With 95% confidence we can say that the average number of adults that live with one or more chronic conditions lies between 42.648 and 47.352 


## 3 

The nutrition label on a bag of potato chips says that a one ounce (28 gram) serving of potato chips has 130 calories and contains ten grams of fat, with three grams of saturated fat. A random sample of 35 bags yielded a sample mean of 136 calories with a standard deviation of 17 calories.

a) Write down the null and alternative hypotheses for a two-sided test of whether the nutrition label is lying.

Null Hypothesis : The mean of a one ounce serving of potato chips is 130 calories 

Alternate Hypothesis : The mean of a one ounce seving of potato chips is greater or lesser than 130 calories 

b) (4 pt) Calculate the test statistic and find the p value.

Assuming the null hypothesis to be true : u  = 130 
x_bar = 136 
n = 35
sd = 17 

Z = x_bar - u / ( sd / sqrt(n) )

```{r 3.b}
u  = 130 
x_bar = 136 
n = 35
sd = 17 
num <- x_bar - u 
den <- sd / sqrt(n)
Z <- num /den 
2*pnorm(-abs(Z))
```

c) (2 pt) If you were the potato chip company would you rather have your alpha = 0.05 or
0.025 in this case? Why?

Since the p-value is 0.036, if the significance level is 0.05, the p-value is lesser than the significance level, therefore we would have to reject the null hypothesis that the calories in one serving is 130 calories and accept that it is more than 130 calories. However if the significance level is 0.025 we would be within the confidence interval and accept the null hypothesis. But that being said, we cannot assume the significance level after calculating or taking the samples as this is ethically not correct. 


## 4 

Regression was originally used by Francis Galton to study the relationship between parents and children. He wondered if he could predict a man’s height based on the height of his father? This is the question we will explore in this problem. You can obtain data similar to that used by Galton as follows:

```{r 4, include = FALSE}
library(UsingR)
height <- get("father.son")

```

a) (5 pt) Perform an exploratory analysis of the father and son heights. What does the relationship look like? Would a linear model be appropriate here?
```{r 4.a.1}
summary(height)
```
We see a summary of the dataset initially
```{r 4.a.2}
plot(sheight ~ fheight, data = height, xlab = "Father's height", ylab="Son's height" )
title("Plot of Father-Son Height")
cor(height$sheight, height$fheight)
```
We can see that there is a positive correlation between the son and father's height, therefore a linear model could be utilized here. Let us also observe the residuals just to ensure there isn't a pattern. 

```{r 4.a.3}
fit <- lm(sheight ~ fheight, data = height)
par(mfrow = c(2, 2))
plot(fit)
```


We see different kinds of plots to observe this data. Let us focus on the residual plot to the top left. From this plot we can observe that the variables are randomly distrbuted and that there is no pattern between the residuals and the independent variable. Thus we can say that a linear model is appropriate to use. If there was a visible pattern ( like u-shaped or inverted-u) in this plot we would have to utilize non-linear models.

```{r 4.a.4}
predicted <- predict(fit)
residuals <- resid(fit)
ggplot(height, aes(x = fheight, y = sheight)) +
  geom_smooth(method = "lm", se = FALSE, color = "lightgrey") + 
  geom_segment(aes(xend = fheight, yend = predicted), alpha = .2) + 
  geom_point(aes(alpha = abs(residuals))) + 
  guides(alpha = FALSE) +
  geom_point(aes(y = predicted), shape = 1) +
  theme_bw()  
```

We can see how the further the further the actual data point is from the regression line, the darker is the point 
b) (5 pt) Use the lm function in R to fit a simple linear regression model to predict son’s
height as a function of father’s height. Write down the model,

ysheight = β0 + β1 x fheight

filling in estimated coefficient values and interpret the coefficient estimates.

```{r 4.b}
mod3 <- lm(height$sheight ~ height$fheight)
summary(mod3)
```


Therefore β0 = 33.886
          β1 = 0.5149

The estimates of the slope means that for an increase in every inch of the father's height, the son's height increase by 0.51409 inches. The estimate of the intercept means that if the father's height is zero the son's height is 33.86 inches which doesn't make sense. There would've been no data where the father's height is zero. However it is acceptable to interpret this as a coefficient in the model. 
          
c) (5 pt) Find the 95% confidence intervals for the estimates. You may find the confint() command useful.
```{r 4.c}
confint(mod3,level=0.95)
```



d) (5 pt) Produce a visualization of the data and the least squares regression line.

```{r 4.d}
library(ggplot2)
g<- ggplot(data = height,mapping = aes(x= fheight, y = sheight))
g <- g + geom_point() + xlab("Father's Height") + ylab("Son's Heights")
g <- g + labs(title = "Plot of Father- Son Height")
g <- g + geom_smooth(method = 'lm', se = TRUE)
g
```


e) (5 pt) Produce a visualization of the residuals versus the fitted values. (You can inspect the elements of the linear model object in R using names()). Discuss what you see. Do you have any concerns about the linear model?
```{r 4.e}
fit <- lm(sheight ~ fheight, data = height)
res <- resid(fit)
par(mfrow = c(1,1))
plot(height$fheight, res, 
     ylab = "Residuals",
     xlab = "Father's Height ( Fitted Values)",
     main = "Residuals of Height")
abline(0, 0)
 
```

As mentioned in the exploratory analysis section, since the plot of fitted values vs the residuals is random, a linear model is appropriate to use. 

f) (5 pt) Using the model you fit in part (b) predict the height of 5 males whose father are 50, 55, 70, 75, and 90 inches respectively. You may find the predict() function helpful.

```{r 4.f}
values <- data.frame(fheight=c(50,55,70,75,90))
predictions <-  predict(fit, newdata = values)
val <- as.numeric(c(50,55,70,75,90))
m <- ggplot(mapping = aes(x = val, y =predictions))
m <- m + geom_point() + labs(title = "Predicting Son's Height") + xlab("Father's Height")+ ylab("Prediction of Son's Height")
m
```

g) (5 pt) What do the estimates of the slope and height mean? Are the results statistically significant? Are they practically significant?

```{r 4.g}
summary(fit)
```

The estimates of the slope and height are statistically significant as the p-value for both ( < 2e-16 ) is very small. Indicating that the possibility of observing this by chance is very small. The estimates of the slope means that for an increase in every inch of the father's height, the son's height increase by 0.51409 inches. The estimate of the intercept means that if the father's height is zero the son's height is 33.86 inches which doesn't make sense. There would've been no data where the father's height is zero. However it is acceptable to interpret this as a coefficient in the model. 

## 5 



An investigator is interested in understanding the relationship, if any, between the analytical skills of young gifted children and the father’s IQ, the mother’s IQ, and hours of educational TV. The data are here:

```{r 5, include = FALSE}
library(openintro)
data(gifted)
```

```{r 5.a.1}
mod1 <- lm(fatheriq ~ score, data = gifted)
summary(mod1)
```

```{r 5.a.2}
mod2 <- lm(motheriq ~ score, data = gifted)
summary(mod2)
```

b) (5 pt) What are the estimates of the slopes for father and mother’s IQ score with their 95% confidence intervals? (Note, estimates and confidence intervals are usually reported:
Estimate (95% CI: CIlower, CIupper)

```{r 5.b.1}
est_conf <- confint(mod1)
est_conf
```

Intercept Estimate:
92.2717 (95% CI : 51.294, 133.249)
Score Estimate: 
0.1414 (95% CI : -0.116, 0.398)

```{r 5.b.2}
est_conf_2 <- confint(mod2)
est_conf_2
```

Intercept Estimate:
-9.5521 (95% CI : -73.537, 54.432)
Score Estimate: 
0.8026 (95% CI : 0.400, 1.204)

c) (5 pt) How are these interpreted?
Assuming the null hypothesis is true, we can say with 95% confidence that the true intercept for the model of analytical skills of a young child and his/her father will lie between 51.294 and 133.249 and the true slope would lie between -0.116 and 0.398

Similarly,
Assuming the null hypothesis is true, we can say with 95% confidence that the true intercept for the model of analytical skills of a young child and his/her mother will lie between -73.537 and 54.432 and the true slope would lie between 0.400 and 1.204


d) (5 pt) What conclusions can you draw about the association between the child’s score and the mother and father’s IQ?

Looking at the Adjusted R-squared in both models we can see that the Adjusted R-squared for the model fitting the association between the father's IQ and the child's score is 0.007 whereas the Adjusted R-squared for the model fitting the association between the mother's IQ and the child's score is 0.3065. Thus the adjusted r-squared for the model fitting the association betweeen the mother's IQ and the child's score is greater and statistically more significant than that between the father's. Thus we can interpret that the mother's IQ is a better predictor of the analytical skills of a young child 

```{r 5.d.1, include= FALSE}
require(gridExtra)
```

```{r 5.d.2}
g1<- ggplot(data = gifted,mapping = aes(x= fatheriq, y = score))
g1 <- g1 + geom_point() + xlab("Father's IQ") + ylab("Child's Score")
g1 <- g1 + labs(title = "Plot of analytical skills of child")
g1 <- g1 + geom_smooth(method = 'lm', se = TRUE)


g2<- ggplot(data = gifted,mapping = aes(x= motheriq, y = score))
g2 <- g2 + geom_point() + xlab("Mother's IQ") + ylab("Child's Score")
g2<- g2 + labs(title = "Plot of analytical skills of child")
g2 <- g2 + geom_smooth(method = 'lm', se = TRUE)


grid.arrange(g1, g2, ncol = 2)
```

