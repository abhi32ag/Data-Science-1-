---
title: "Lab_4"
author: "Abhinav Garg"
date: "2/16/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("/Users/abhi/Documents/UW/Courses/Winter_Quarter_17/INFX_573/Week_6")
```

## Load Data
```{r Load}
data <- read.csv("census_data.csv")
data$income.g50 <- rep(0, nrow(data))
data$income.g50[data$income==" >50K"] <- 1
```

## Exploring Relationships

```{r Exploring}
mod <- glm(income.g50 ~ education + age + sex + race,
data=data[,!colnames(data)%in%"income"], family="binomial")
```

```{r Exploring.a}
summary(mod)
```
## a.) 
The log odds ratio for having a master's degree is 2.91 
and for having 1st to 4th grade education is -0.18

The log odds ratio for having a master's degree is `r exp(2.91)` 
and for having 1st to 4th grade education is `r exp(-0.18)`

The p-value for Master's degree is < 2e -16 hence it is statistically significant but the p-value for 1st - 4th grade education is 0.78371 hence there is more probability that this is by chance. 

`r 0.5/22` is the adjusted alpha level. So we can see that the p-value for Master's is lower than 0.0227273 so it is significant however education 1st -4th is 0.78371 hence it isn't as signficant 

## b.)
P-values for both age and sex are < 2e-16 therefore they are both statistically significant and are practically significant as age and sex do determine a person's income 



## 3
```{r Exploring.b}

x <- data$age
plot(x, data$income.g50, col="blue")
fits <- fitted(mod)
points(x, fits, pch=19, cex=0.3)
```

The predicted outcomes are so variable because although age is on the x-axis the other variables vary and that's why we don't have just one sigmoid line 



## 4
```{r Explore prob}

tab <- table(data$income.g50, fits>=0.25)
(tab[1,2]+tab[2,1])/sum(tab)

tab <- table(data$income.g50, fits>=0.5)
(tab[1,2]+tab[2,1])/sum(tab)

tab <- table(data$income.g50, fits>=0.75)
(tab[1,2]+tab[2,1])/sum(tab)

```

The cutoff with 0.5 has lowest percent error 



## 5
```{r Examine}
library(AUC)
y <- factor(data$income.g50)
rr <- roc(fits, y)
plot(rr)
auc(rr)
```


b. How well does it fit?

The model fits well as the AUC is 0.802 


## 6

a.
```{r Formulate}
mod <- glm(income.g50~.,
data=data[,!colnames(data)%in%c("income")], family="binomial")
summary(mod)
```

The same patterns are not seen for schooling 


```{r Formulate.b}

x <- data$age
plot(x, data$income.g50, col="blue")
fits <- fitted(mod)
points(x, fits, pch=19, cex=0.3)
```

No the predicted probabilities do not have the same pattern as the other model as we were not considering the additional variables 



c.
```{r Formulate.c}

tab <- table(data$income.g50, fits>=0.25)
(tab[1,2]+tab[2,1])/sum(tab)
tab
tab <- table(data$income.g50, fits>=0.5)
(tab[1,2]+tab[2,1])/sum(tab)

tab <- table(data$income.g50, fits>=0.75)
(tab[1,2]+tab[2,1])/sum(tab)

```

The cutoff is 0.5 for lowest percent error. However we can see that for all three cutoffs this model has a lower percent error than the other model 



d. Plot the ROC and calculate the AUC. Again, does this model out perform the other model?
```{r Formulate.d}
library(AUC)
y <- factor(data$income.g50)
rr <- roc(fits, y)
plot(rr)
auc(rr)
```

This model does outperform the other model as the auc is 0.889 whereas the earlier model is 0.808 

Extra credit (5 points): Run a k-fold validation on both models and decide which you would prefer to use for predicting high income.


```{r cross-validation}
library(caret)
set.seed(10)
mod1 <- createDataPartition(data$income.g50, p =0.6, list = FALSE)
mod1_training <- data[mod1,]
mod1_testing <- data[ -mod1, ]

mod1_fit <- train(income.g50 ~ education + age + sex + race,
data=mod1_training[,!colnames(data)%in%"income"], method="glm", family="binomial")

summary(mod1_fit)

set.seed(10)
mod2 <- createDataPartition(data$income.g50, p =0.6, list = FALSE)
mod2_training <- data[mod1,]
mod2_testing <- data[ -mod1, ]

mod2_fit <- train(income.g50~.,
data=data[,!colnames(data)%in%c("income")], method="glm", family="binomial")

summary(mod2_fit)

require(glmnet)

ctrl1 <- trainControl(method='cv', number = 10, savePredictions= TRUE)
mod1_fit <- train(income.g50 ~ education + age + sex + race,
data=mod1_training[,!colnames(data)%in%"income"], method="glmnet", family="binomial", trControl = ctrl1, tuneLength = 5)
mod1_fit


require(glmnet)

ctrl2 <- trainControl(method='cv', number = 10, savePredictions= TRUE)
mod2_fit <- train(income.g50 ~ .,
data=mod2_training[,!colnames(data)%in%"income"], method="glmnet", family="binomial", trControl = ctrl2, tuneLength = 5)
mod2_fit
```

Thus we can see that after performing k cross validation the second model has a better R-squared on most predictors than the first model 


